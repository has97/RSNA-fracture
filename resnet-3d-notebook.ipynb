{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Downloading Libraries","metadata":{}},{"cell_type":"code","source":"!pip install python-gdcm\n!pip install -U pylibjpeg[all]\n!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:44:43.468861Z","iopub.execute_input":"2022-11-25T05:44:43.469731Z","iopub.status.idle":"2022-11-25T05:45:19.566440Z","shell.execute_reply.started":"2022-11-25T05:44:43.469639Z","shell.execute_reply":"2022-11-25T05:45:19.565134Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting python-gdcm\n  Downloading python_gdcm-3.0.20-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: python-gdcm\nSuccessfully installed python-gdcm-3.0.20\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pylibjpeg[all]\n  Downloading pylibjpeg-1.4.0-py3-none-any.whl (28 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pylibjpeg[all]) (1.21.6)\nCollecting pylibjpeg-libjpeg\n  Downloading pylibjpeg_libjpeg-1.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hCollecting pylibjpeg-rle\n  Downloading pylibjpeg_rle-1.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.3/969.3 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pylibjpeg-openjpeg\n  Downloading pylibjpeg_openjpeg-1.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pylibjpeg-rle, pylibjpeg-openjpeg, pylibjpeg-libjpeg, pylibjpeg\nSuccessfully installed pylibjpeg-1.4.0 pylibjpeg-libjpeg-1.3.2 pylibjpeg-openjpeg-1.2.1 pylibjpeg-rle-1.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Importing and Installing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport pathlib\nimport random\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import optim\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport albumentations.pytorch\nfrom tqdm import tqdm\n# import pytorch_lightning as pl\n# from pytorch_lightning.callbacks import ModelCheckpoint\nfrom torchsummary import summary\nfrom sklearn.metrics import multilabel_confusion_matrix\nfrom sklearn.model_selection import StratifiedShuffleSplit\n# Setting Seed for reproducibility\nseed = 42\nrandom.seed(seed)\ntorch.manual_seed(seed)  \ntorch.cuda.manual_seed(seed)  \ntorch.cuda.manual_seed_all(seed)  \ntorch.backends.cudnn.deterministic = True\n\nprint(f\"Torch Version: {torch.__version__}\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:20.482871Z","iopub.execute_input":"2022-11-25T05:47:20.483555Z","iopub.status.idle":"2022-11-25T05:47:22.443778Z","shell.execute_reply.started":"2022-11-25T05:47:20.483512Z","shell.execute_reply":"2022-11-25T05:47:22.442726Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Torch Version: 1.11.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Hyperparamters","metadata":{}},{"cell_type":"code","source":"epochs = 4\nbatch_size = 2\nnumber_of_slices = 160 # Mean of, mean and median of no of slices\ninput_shape = (224, 224)\nresize_size = 256\nnum_of_hidden = 1 # Not Needed\nhidden_dimension = [] # Not needed\noutput_categories = 8\nloss_weights = {'-ve': torch.tensor([1., 1., 1., 1., 1., 1., 1., 7.]),\n                '+ve': torch.tensor([2., 2., 2., 2., 2., 2., 2., 14.])}\ninput_channels = 1\nhidden_cnn_channels = [64, 64, 64, 128, 128, 128, 128, 256, 256, 256, 256, 256, 256, 512, 512, 512]\nblock_cnn_layers = [2 for i in range(16)]\nnum_cnn_block = 16\ncriterion = nn.BCEWithLogitsLoss(reduction = 'none')\ntrain_size = 0.8\nval_size = 0.2\nlr = 0.00005\nsave_path = '/kaggle/working/'","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:22.445777Z","iopub.execute_input":"2022-11-25T05:47:22.446379Z","iopub.status.idle":"2022-11-25T05:47:22.454172Z","shell.execute_reply.started":"2022-11-25T05:47:22.446340Z","shell.execute_reply":"2022-11-25T05:47:22.453136Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Helper Function","metadata":{}},{"cell_type":"code","source":"def loss_fxn(y_true, y_pred):\n    losses = nn.BCEWithLogitsLoss()\n    loss = losses(y_true,y_pred)\n    weights  = y_true*loss_weights['+ve'].to(device) + (1-y_true)*loss_weights['-ve'].to(device)\n    loss = (loss * weights).sum(axis=1)\n    loss = loss.mean()\n    loss = (loss / weights.sum(axis=1)).sum()\n    return loss\n\ndef metric(y_true, y_pred):\n    losses = nn.BCEWithLogitsLoss()\n    loss = losses(y_true,y_pred)\n    weights  = y_true*loss_weights['+ve'].to(device) + (1-y_true)*loss_weights['-ve'].to(device)\n    loss = (loss * weights).sum(axis=1)\n    loss = loss.mean()\n    loss = (loss / weights.sum(axis=1)).sum()\n    return loss\n\ndef load_model(model, optimizer = None, save_name = 'best_model.pth'):\n    checkpoint = torch.load(save_name)\n    model.load_state_dict(checkpoint['model'])\n    if 'optimizer' in checkpoint and optimizer:\n        optimizer.load_state_dict(checkpoint[optimizer])\n    return model\n\ndef save_model(model, optimizer = None, save_name = 'best_model.pth'):\n    save_dictionary ={}\n    save_dictionary['model'] = model.state_dict()\n    if optimizer:\n        save_dictionary['optimizer'] = optimizer.state_dict()\n    torch.save(save_dictionary, save_name)\n\n\ndef evaluate(model, dataloader):\n    model.eval()\n    with torch.no_grad():\n        metrics = 0.0\n        losses = 0.0\n        for images, labels in tqdm(dataloader):\n            images = images.to(device)\n            labels = labels.to(device)\n            with torch.cuda.amp.autocast():\n                preds = model(images)\n                loss = loss_fxn(preds, labels)\n            met = metric(preds, labels)\n            metrics += met.item()\n            losses += loss.item()\n            \n            gc.collect()\n            torch.cuda.empty_cache()\n        final_loss = losses / len(dataloader)\n        final_metric = metrics / len(dataloader)\n        return final_loss, final_metric","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:22.739030Z","iopub.execute_input":"2022-11-25T05:47:22.739612Z","iopub.status.idle":"2022-11-25T05:47:22.753284Z","shell.execute_reply.started":"2022-11-25T05:47:22.739578Z","shell.execute_reply":"2022-11-25T05:47:22.752296Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"base_path = pathlib.Path('/kaggle/input/rsna-2022-cervical-spine-fracture-detection')\ndf = pd.read_csv(base_path/'train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:23.052925Z","iopub.execute_input":"2022-11-25T05:47:23.053614Z","iopub.status.idle":"2022-11-25T05:47:23.085492Z","shell.execute_reply.started":"2022-11-25T05:47:23.053577Z","shell.execute_reply":"2022-11-25T05:47:23.084438Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"            StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7\n0   1.2.826.0.1.3680043.6200                1   1   1   0   0   0   0   0\n1  1.2.826.0.1.3680043.27262                1   0   1   0   0   0   0   0\n2  1.2.826.0.1.3680043.21561                1   0   1   0   0   0   0   0\n3  1.2.826.0.1.3680043.12351                0   0   0   0   0   0   0   0\n4   1.2.826.0.1.3680043.1363                1   0   0   0   0   1   0   0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>patient_overall</th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>C4</th>\n      <th>C5</th>\n      <th>C6</th>\n      <th>C7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.6200</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.27262</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.21561</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.12351</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.1363</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df = df[df['StudyInstanceUID'] != '1.2.826.0.1.3680043.20574'].copy()\nprint(len(df))","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:23.223521Z","iopub.execute_input":"2022-11-25T05:47:23.223882Z","iopub.status.idle":"2022-11-25T05:47:23.236984Z","shell.execute_reply.started":"2022-11-25T05:47:23.223851Z","shell.execute_reply":"2022-11-25T05:47:23.235874Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"2018\n","output_type":"stream"}]},{"cell_type":"code","source":"df['path'] = list(map(lambda x: base_path/'train_images'/x, df['StudyInstanceUID']))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:23.349016Z","iopub.execute_input":"2022-11-25T05:47:23.350385Z","iopub.status.idle":"2022-11-25T05:47:23.384332Z","shell.execute_reply.started":"2022-11-25T05:47:23.350349Z","shell.execute_reply":"2022-11-25T05:47:23.383261Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"            StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7  \\\n0   1.2.826.0.1.3680043.6200                1   1   1   0   0   0   0   0   \n1  1.2.826.0.1.3680043.27262                1   0   1   0   0   0   0   0   \n2  1.2.826.0.1.3680043.21561                1   0   1   0   0   0   0   0   \n3  1.2.826.0.1.3680043.12351                0   0   0   0   0   0   0   0   \n4   1.2.826.0.1.3680043.1363                1   0   0   0   0   1   0   0   \n\n                                                path  \n0  /kaggle/input/rsna-2022-cervical-spine-fractur...  \n1  /kaggle/input/rsna-2022-cervical-spine-fractur...  \n2  /kaggle/input/rsna-2022-cervical-spine-fractur...  \n3  /kaggle/input/rsna-2022-cervical-spine-fractur...  \n4  /kaggle/input/rsna-2022-cervical-spine-fractur...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>patient_overall</th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>C4</th>\n      <th>C5</th>\n      <th>C6</th>\n      <th>C7</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.6200</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/rsna-2022-cervical-spine-fractur...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.27262</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/rsna-2022-cervical-spine-fractur...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.21561</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/rsna-2022-cervical-spine-fractur...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.12351</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/rsna-2022-cervical-spine-fractur...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.1363</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/rsna-2022-cervical-spine-fractur...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# min_slices = 1500\n# max_slices = 0\n# mean_slices = 0\n# count = 0\n# slices_val = []\n# for i in df['path']:\n#     no_of_images = len(list(i.glob(\"*\")))\n#     count+=1\n#     slices_val.append(no_of_images)\n#     mean_slices += no_of_images\n#     if min_slices > no_of_images:\n#         min_slices = no_of_images\n#     if no_of_images > max_slices:\n#         max_slices = no_of_images\n# print(\"Minimum Slices in an Image\", min_slices)\n# print(\"Maximum Slices in an Image\", max_slices)\n# print(\"Mean Slices in an Image\", mean_slices / count)\n# slices_val.sort()\n# print(\"Median Slices in an Image\", slices_val[(count + 1)//2])","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:23.538588Z","iopub.execute_input":"2022-11-25T05:47:23.538925Z","iopub.status.idle":"2022-11-25T05:47:23.544269Z","shell.execute_reply.started":"2022-11-25T05:47:23.538896Z","shell.execute_reply":"2022-11-25T05:47:23.543138Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"strat = StratifiedShuffleSplit(n_splits=2, test_size = val_size/(train_size + val_size), \n                                random_state=seed)\nfor (train_idx, valid_idx) in strat.split(df.index, df['C3']):\n    valid_data = df.iloc[valid_idx]\n    train_data = df.iloc[train_idx]","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:23.697660Z","iopub.execute_input":"2022-11-25T05:47:23.698004Z","iopub.status.idle":"2022-11-25T05:47:23.710468Z","shell.execute_reply.started":"2022-11-25T05:47:23.697973Z","shell.execute_reply":"2022-11-25T05:47:23.709526Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"len(train_data), len(valid_data)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:23.844900Z","iopub.execute_input":"2022-11-25T05:47:23.845556Z","iopub.status.idle":"2022-11-25T05:47:23.852121Z","shell.execute_reply.started":"2022-11-25T05:47:23.845520Z","shell.execute_reply":"2022-11-25T05:47:23.851123Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(1614, 404)"},"metadata":{}}]},{"cell_type":"code","source":"train_data['patient_overall'].value_counts(normalize = True)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:24.000353Z","iopub.execute_input":"2022-11-25T05:47:24.002259Z","iopub.status.idle":"2022-11-25T05:47:24.013105Z","shell.execute_reply.started":"2022-11-25T05:47:24.002229Z","shell.execute_reply":"2022-11-25T05:47:24.012105Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0    0.521066\n1    0.478934\nName: patient_overall, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"valid_data['patient_overall'].value_counts(normalize = True)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:24.163964Z","iopub.execute_input":"2022-11-25T05:47:24.164631Z","iopub.status.idle":"2022-11-25T05:47:24.174369Z","shell.execute_reply.started":"2022-11-25T05:47:24.164594Z","shell.execute_reply":"2022-11-25T05:47:24.173141Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0    0.537129\n1    0.462871\nName: patient_overall, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"class CervicalDataset(Dataset):\n    def __init__(self, df, no_of_slice, prob = 0.5, test = False, tta = False):\n        '''\n        '''\n        self.df = df\n        self.test = test\n        self.tta = tta\n        self.prob = prob\n        self.no_of_slice = no_of_slice\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        data = self.df['path'].iloc[idx]\n        images_path = list(data.glob(\"*\"))\n        no_of_images = len(images_path)\n        transforms_list = [A.LongestMaxSize(max_size=resize_size, interpolation=1),\n                           A.PadIfNeeded(min_height=input_shape[0], min_width=input_shape[1],\n                                         border_mode=0, value=(0,0,0))]\n        if not(self.test):\n            value = np.random.uniform()\n            if value >= self.prob: \n                transforms_list.append(A.HorizontalFlip(always_apply = True))\n                \n            value = np.random.uniform()\n            if value >= self.prob:\n                transforms_list.append(A.ShiftScaleRotate(shift_limit = 0.15,\n                                                          border_mode = 2, always_apply = True))\n            \n            # Other Transformation to add: Random Brightness, Contrast, Clahe, Scale Intensity \n                \n            transforms_list.append(A.CenterCrop(height = input_shape[0], width = input_shape[1]))\n            transforms = A.Compose(transforms_list)\n        elif self.test and self.tta:\n            pass\n        if self.test:\n            transforms_list.append(A.CenterCrop(height = input_shape[0], width = input_shape[1]))\n            transforms = A.Compose(transforms_list)\n            \n        imgs = []\n        for i in range(1, len(images_path)+1):\n            path = images_path[0].parent/f\"{i}.dcm\"\n            \n            try:\n                data = pydicom.dcmread(path)\n#                 data.PhotometricInterpretation = 'YBR_FULL'\n                img_data = apply_voi_lut(data.pixel_array, data)\n                img_data = img_data - np.min(img_data)\n                if np.max(img_data) != 0:\n                    img_data = img_data / np.max(img_data)\n    #             data = (data * 255).astype(np.uint8)\n                album = transforms(image = img_data)\n                img_data = album['image']\n            except FileNotFoundError:\n                continue\n            imgs.append(img_data)\n            if len(imgs) > self.no_of_slice:\n                break\n            \n        if len(imgs) > self.no_of_slice:\n            imgs = imgs[:self.no_of_slice]\n        \n        if len(imgs) < self.no_of_slice:\n            imgs.extend([np.zeros((input_shape[0], input_shape[1]))\n                         for i in range(self.no_of_slice - len(imgs))])\n\n        imgs = np.array(imgs)\n        imgs = torch.from_numpy(imgs).float()\n        labels = torch.as_tensor(self.df.iloc[idx].iloc[1:-1]).float()\n        imgs = torch.unsqueeze(imgs, dim = 0)\n        return imgs, labels","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:24.337628Z","iopub.execute_input":"2022-11-25T05:47:24.338449Z","iopub.status.idle":"2022-11-25T05:47:24.356314Z","shell.execute_reply.started":"2022-11-25T05:47:24.338410Z","shell.execute_reply":"2022-11-25T05:47:24.355298Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_dataset = CervicalDataset(train_data, no_of_slice = number_of_slices)\nvalid_dataset = CervicalDataset(valid_data, no_of_slice = number_of_slices, test = True)\ntrainloader = DataLoader(train_dataset, batch_size = batch_size, num_workers = 2)\nvalidloader = DataLoader(valid_dataset, batch_size = batch_size, num_workers = 2)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:24.486776Z","iopub.execute_input":"2022-11-25T05:47:24.487143Z","iopub.status.idle":"2022-11-25T05:47:24.492228Z","shell.execute_reply.started":"2022-11-25T05:47:24.487113Z","shell.execute_reply":"2022-11-25T05:47:24.491195Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# # Visualize\n# image = imgs[3]\n# print(image.shape) # No_of_slice x height x width\n# fig, axes = plt.subplots(nrows = 1, ncols = 4, figsize = (15,10))\n# axes[0].imshow(image.numpy().mean(axis = 0), cmap = 'gray')\n# axes[0].axis('off')\n# axes[1].imshow(image.numpy()[:, :, image.shape[2]//2], cmap = 'gray')\n# axes[1].axis('off')\n# axes[2].imshow(image.numpy()[:, 128, :], cmap = 'gray')\n# axes[2].axis('off')\n# axes[3].imshow(image.numpy()[6, :, :], cmap = 'gray')\n# axes[3].axis('off');","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:24.670532Z","iopub.execute_input":"2022-11-25T05:47:24.670887Z","iopub.status.idle":"2022-11-25T05:47:24.675717Z","shell.execute_reply.started":"2022-11-25T05:47:24.670855Z","shell.execute_reply":"2022-11-25T05:47:24.674719Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"class ResnetBlock(nn.Module):\n    def __init__(self, input_channels, output_channels, stride = 1, num_cnn = 2, skip = False):\n        super(ResnetBlock, self).__init__()\n        layers = []\n        layers.append(nn.Conv3d(in_channels = input_channels, out_channels = output_channels, \n                               kernel_size = 3, padding = 1, stride = stride))\n        layers.append(nn.ReLU())\n        # Change after 1st layer\n        layers.append(nn.Conv3d(in_channels = output_channels, out_channels = output_channels, \n                               kernel_size = 3, padding = 1, stride = 1))\n        layers.append(nn.ReLU())\n            \n        self.base_module = nn.Sequential(*layers)\n        self.skip = skip\n        if self.skip:\n            self.skip_connection = nn.Conv3d(in_channels = input_channels, out_channels = output_channels,\n                                            kernel_size = 1, stride = stride)\n        self.__initialise_weights()\n        \n    def __initialise_weights(self):\n        for m in self.base_module:\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight)\n              \n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n                    \n    def forward(self, x):\n        out = self.base_module(x)\n        if self.skip:\n            x = self.skip_connection(x)\n        out = out + x\n        return out\n    \nclass Resnet(nn.Module):\n    def __init__(self, input_channels, output_categories,\n                 hidden_channels, block_cnn_layers , num_cnn_block = 5, learning_rate = lr):\n        super(Resnet, self).__init__()\n        \n        hidden_cnn_channels = hidden_channels[:]\n        if (num_cnn_block != len(hidden_cnn_channels)):\n            raise \"Number of Hidden layer and length of hidden dim must be same\"\n\n        self.learning_rate = learning_rate\n        \n        \n        # Dropout probability\n        self.p = 0.5\n        hidden_cnn_channels.insert(0, 64)\n        resnet_layers = []\n        idx = 1\n        resnet_layers.append(nn.Conv3d(in_channels = input_channels, out_channels = 64,\n                                      kernel_size = 7, stride = 2))\n        resnet_layers.append(nn.MaxPool3d(kernel_size = 2, stride = 2))\n        \n        for i in range(1,len(hidden_cnn_channels)):\n            if hidden_cnn_channels[i-1] == hidden_cnn_channels[i]:\n                resnet_layers.append(ResnetBlock(hidden_cnn_channels[i-1], \n                                           hidden_cnn_channels[i]))\n            else:\n                resnet_layers.append(ResnetBlock(hidden_cnn_channels[i-1], \n                                                hidden_cnn_channels[i],\n                                                stride = 2, skip = True))\n                \n        resnet_layers.append(nn.AdaptiveAvgPool3d((7,1,1)))\n        self.network = nn.Sequential(*resnet_layers)\n        \n        # 125440 is number of output from CNN\n#         hidden_dim.insert(0, 3584)\n\n        self.classification = nn.Sequential(nn.Linear(3584, output_categories))\n        self.__initialise_weights()\n\n    def __initialise_weights(self):\n        for m in self.classification:\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight)\n              \n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n        \n    def forward(self,x):\n        out = self.network(x)\n        out = out.view(batch_size, -1)\n        out = self.classification(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:25.212280Z","iopub.execute_input":"2022-11-25T05:47:25.212633Z","iopub.status.idle":"2022-11-25T05:47:25.228941Z","shell.execute_reply.started":"2022-11-25T05:47:25.212604Z","shell.execute_reply":"2022-11-25T05:47:25.227782Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def fit(model, n_epochs, trainloader, validloader, \n        optimizer, scheduler = None, early_stopping = False, \n        early_stopping_threshold = 5, min_metric_score = 100,\n        save_path = 'best_model.pth'):\n    model = model.to(device)\n    train_losses = []\n    train_metric = []\n    valid_losses = []\n    valid_metric = []\n    scaler = torch.cuda.amp.GradScaler()\n    no_change = 0\n\n    for epoch in range(n_epochs):\n        loop = tqdm(enumerate(trainloader), total = len(trainloader), leave = False)\n        metric_scores = 0.0\n        losses = 0.0\n        model.train()\n        for idx, (images, labels) in loop:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # For FP16 (Ref: https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/)\n            with torch.cuda.amp.autocast():\n                preds = model(images)\n                loss = loss_fxn(preds, labels)\n\n            optimizer.zero_grad()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            gc.collect()\n\n            metric_score = metric(preds, labels)\n            metric_scores += metric_score.item()\n            losses += loss.item()\n            loop.set_description(f\"[{epoch}/{n_epochs}]\")\n            loop.set_postfix(train_loss = loss.item(), train_metric = metric_score.item())\n            \n            torch.cuda.empty_cache()\n            \n        \n        batch_loss_train = losses/len(trainloader)\n        batch_metric_train = metric_scores/len(trainloader)\n        train_losses.append(batch_loss_train)\n        train_metric.append(batch_metric_train)\n\n        valid_loss, valid_score = evaluate(model, validloader)\n        \n        valid_losses.append(valid_loss)\n        valid_metric.append(valid_score)\n        print(f\"[{epoch}/{n_epochs}]\")\n        print(f\"train_loss = {batch_loss_train}, train_metric = {batch_metric_train}, valid_loss = {valid_loss}, valid_metric = {valid_score}\")\n        if scheduler:\n            scheduler.step()\n        if valid_score < min_metric_score:\n            print(\"<<<< Saving >>>>\")\n            no_change = 0\n            min_dice_score = valid_score\n            save_model(model, optimizer, save_path)\n\n        else:\n            no_change+=1\n        \n        if no_change > early_stopping_threshold and early_stopping:\n            print('###### Early Stopping ######')\n            break\n    return train_losses, train_metric, valid_losses, valid_metric","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:47:25.523455Z","iopub.execute_input":"2022-11-25T05:47:25.523804Z","iopub.status.idle":"2022-11-25T05:47:25.536053Z","shell.execute_reply.started":"2022-11-25T05:47:25.523772Z","shell.execute_reply":"2022-11-25T05:47:25.535136Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model = Resnet(input_channels, output_categories, hidden_cnn_channels, block_cnn_layers, num_cnn_block).to(device)\n\n# print(\"Model's state_dict:\")\n# for param_tensor in model.state_dict():\n#     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n    \n# def count_parameters(model):\n#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# print(f'The model has {count_parameters(model):,} trainable parameters')\nsummary(model,(1,160,224,224))","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:48:39.049926Z","iopub.execute_input":"2022-11-25T05:48:39.050615Z","iopub.status.idle":"2022-11-25T05:48:49.253279Z","shell.execute_reply.started":"2022-11-25T05:48:39.050576Z","shell.execute_reply":"2022-11-25T05:48:49.252135Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv3d-1     [-1, 64, 77, 109, 109]          22,016\n         MaxPool3d-2       [-1, 64, 38, 54, 54]               0\n            Conv3d-3       [-1, 64, 38, 54, 54]         110,656\n              ReLU-4       [-1, 64, 38, 54, 54]               0\n            Conv3d-5       [-1, 64, 38, 54, 54]         110,656\n              ReLU-6       [-1, 64, 38, 54, 54]               0\n       ResnetBlock-7       [-1, 64, 38, 54, 54]               0\n            Conv3d-8       [-1, 64, 38, 54, 54]         110,656\n              ReLU-9       [-1, 64, 38, 54, 54]               0\n           Conv3d-10       [-1, 64, 38, 54, 54]         110,656\n             ReLU-11       [-1, 64, 38, 54, 54]               0\n      ResnetBlock-12       [-1, 64, 38, 54, 54]               0\n           Conv3d-13       [-1, 64, 38, 54, 54]         110,656\n             ReLU-14       [-1, 64, 38, 54, 54]               0\n           Conv3d-15       [-1, 64, 38, 54, 54]         110,656\n             ReLU-16       [-1, 64, 38, 54, 54]               0\n      ResnetBlock-17       [-1, 64, 38, 54, 54]               0\n           Conv3d-18      [-1, 128, 19, 27, 27]         221,312\n             ReLU-19      [-1, 128, 19, 27, 27]               0\n           Conv3d-20      [-1, 128, 19, 27, 27]         442,496\n             ReLU-21      [-1, 128, 19, 27, 27]               0\n           Conv3d-22      [-1, 128, 19, 27, 27]           8,320\n      ResnetBlock-23      [-1, 128, 19, 27, 27]               0\n           Conv3d-24      [-1, 128, 19, 27, 27]         442,496\n             ReLU-25      [-1, 128, 19, 27, 27]               0\n           Conv3d-26      [-1, 128, 19, 27, 27]         442,496\n             ReLU-27      [-1, 128, 19, 27, 27]               0\n      ResnetBlock-28      [-1, 128, 19, 27, 27]               0\n           Conv3d-29      [-1, 128, 19, 27, 27]         442,496\n             ReLU-30      [-1, 128, 19, 27, 27]               0\n           Conv3d-31      [-1, 128, 19, 27, 27]         442,496\n             ReLU-32      [-1, 128, 19, 27, 27]               0\n      ResnetBlock-33      [-1, 128, 19, 27, 27]               0\n           Conv3d-34      [-1, 128, 19, 27, 27]         442,496\n             ReLU-35      [-1, 128, 19, 27, 27]               0\n           Conv3d-36      [-1, 128, 19, 27, 27]         442,496\n             ReLU-37      [-1, 128, 19, 27, 27]               0\n      ResnetBlock-38      [-1, 128, 19, 27, 27]               0\n           Conv3d-39      [-1, 256, 10, 14, 14]         884,992\n             ReLU-40      [-1, 256, 10, 14, 14]               0\n           Conv3d-41      [-1, 256, 10, 14, 14]       1,769,728\n             ReLU-42      [-1, 256, 10, 14, 14]               0\n           Conv3d-43      [-1, 256, 10, 14, 14]          33,024\n      ResnetBlock-44      [-1, 256, 10, 14, 14]               0\n           Conv3d-45      [-1, 256, 10, 14, 14]       1,769,728\n             ReLU-46      [-1, 256, 10, 14, 14]               0\n           Conv3d-47      [-1, 256, 10, 14, 14]       1,769,728\n             ReLU-48      [-1, 256, 10, 14, 14]               0\n      ResnetBlock-49      [-1, 256, 10, 14, 14]               0\n           Conv3d-50      [-1, 256, 10, 14, 14]       1,769,728\n             ReLU-51      [-1, 256, 10, 14, 14]               0\n           Conv3d-52      [-1, 256, 10, 14, 14]       1,769,728\n             ReLU-53      [-1, 256, 10, 14, 14]               0\n      ResnetBlock-54      [-1, 256, 10, 14, 14]               0\n           Conv3d-55      [-1, 256, 10, 14, 14]       1,769,728\n             ReLU-56      [-1, 256, 10, 14, 14]               0\n           Conv3d-57      [-1, 256, 10, 14, 14]       1,769,728\n             ReLU-58      [-1, 256, 10, 14, 14]               0\n      ResnetBlock-59      [-1, 256, 10, 14, 14]               0\n           Conv3d-60      [-1, 256, 10, 14, 14]       1,769,728\n             ReLU-61      [-1, 256, 10, 14, 14]               0\n           Conv3d-62      [-1, 256, 10, 14, 14]       1,769,728\n             ReLU-63      [-1, 256, 10, 14, 14]               0\n      ResnetBlock-64      [-1, 256, 10, 14, 14]               0\n           Conv3d-65      [-1, 256, 10, 14, 14]       1,769,728\n             ReLU-66      [-1, 256, 10, 14, 14]               0\n           Conv3d-67      [-1, 256, 10, 14, 14]       1,769,728\n             ReLU-68      [-1, 256, 10, 14, 14]               0\n      ResnetBlock-69      [-1, 256, 10, 14, 14]               0\n           Conv3d-70         [-1, 512, 5, 7, 7]       3,539,456\n             ReLU-71         [-1, 512, 5, 7, 7]               0\n           Conv3d-72         [-1, 512, 5, 7, 7]       7,078,400\n             ReLU-73         [-1, 512, 5, 7, 7]               0\n           Conv3d-74         [-1, 512, 5, 7, 7]         131,584\n      ResnetBlock-75         [-1, 512, 5, 7, 7]               0\n           Conv3d-76         [-1, 512, 5, 7, 7]       7,078,400\n             ReLU-77         [-1, 512, 5, 7, 7]               0\n           Conv3d-78         [-1, 512, 5, 7, 7]       7,078,400\n             ReLU-79         [-1, 512, 5, 7, 7]               0\n      ResnetBlock-80         [-1, 512, 5, 7, 7]               0\n           Conv3d-81         [-1, 512, 5, 7, 7]       7,078,400\n             ReLU-82         [-1, 512, 5, 7, 7]               0\n           Conv3d-83         [-1, 512, 5, 7, 7]       7,078,400\n             ReLU-84         [-1, 512, 5, 7, 7]               0\n      ResnetBlock-85         [-1, 512, 5, 7, 7]               0\nAdaptiveAvgPool3d-86         [-1, 512, 7, 1, 1]               0\n           Linear-87                    [-1, 8]          28,680\n================================================================\nTotal params: 63,489,800\nTrainable params: 63,489,800\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 30.62\nForward/backward pass size (MB): 1730.45\nParams size (MB): 242.19\nEstimated Total Size (MB): 2003.27\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.AdamW(model.parameters(), lr = lr) \n# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=lr, max_lr=max_lr,\n#                                         step_size_up=10,mode=\"exp_range\",gamma=0.9, cycle_momentum = False)\n\noutput_stats = fit(model, epochs, trainloader, validloader, optimizer, early_stopping = False, \n                   early_stopping_threshold = 2, save_path = f'./resnet3d.pt')","metadata":{"execution":{"iopub.status.busy":"2022-11-23T12:36:46.729254Z","iopub.execute_input":"2022-11-23T12:36:46.731915Z","iopub.status.idle":"2022-11-23T17:27:45.960470Z","shell.execute_reply.started":"2022-11-23T12:36:46.731851Z","shell.execute_reply":"2022-11-23T17:27:45.959248Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"100%|██████████| 202/202 [16:52<00:00,  5.01s/it]                                                 ","output_type":"stream"},{"name":"stdout","text":"[0/4]\ntrain_loss = 2.79634099945814, train_metric = 2.796337214313549, valid_loss = 0.72183658418679, valid_metric = 0.7218489223482585\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 202/202 [12:12<00:00,  3.62s/it]                                               ","output_type":"stream"},{"name":"stdout","text":"[1/4]\ntrain_loss = 0.7451723977978788, train_metric = 0.7451797935747008, valid_loss = 0.7160491490423089, valid_metric = 0.7160536767822681\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 202/202 [12:17<00:00,  3.65s/it]                                              ","output_type":"stream"},{"name":"stdout","text":"[2/4]\ntrain_loss = 0.740032570291186, train_metric = 0.7400322030431128, valid_loss = 0.7149363787162422, valid_metric = 0.7149292519777128\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 202/202 [11:41<00:00,  3.47s/it]                                              ","output_type":"stream"},{"name":"stdout","text":"[3/4]\ntrain_loss = 0.7381946953957557, train_metric = 0.7381931238864287, valid_loss = 0.7145158726685118, valid_metric = 0.7145336230497549\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"save_model(model, save_name='/kaggle/working/resnet3d.pt')","metadata":{"execution":{"iopub.status.busy":"2022-11-23T17:52:01.630421Z","iopub.execute_input":"2022-11-23T17:52:01.631419Z","iopub.status.idle":"2022-11-23T17:52:02.356602Z","shell.execute_reply.started":"2022-11-23T17:52:01.631381Z","shell.execute_reply":"2022-11-23T17:52:02.355586Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"model = load_model(model, save_name = '/kaggle/input/saved-weights/resnet3d.pt')\ny_test=[]\npred_test=[]\nsig = nn.Sigmoid()\nmodel.eval()\nwith torch.no_grad():\n    for data in tqdm(validloader):\n        test_image, test_label=data\n        test_image = test_image.to(device)\n        with torch.cuda.amp.autocast():\n            y_pred = model(test_image)\n            y_pred = sig(y_pred)\n#         print(y_pred)\n        y_pred[y_pred>=0.50]=1\n        y_pred[y_pred<0.50]=0\n        ar1=test_label.cpu().data.numpy()\n        ar2=y_pred.cpu().data.numpy()\n        for x in ar1:\n            y_test.append(x)\n        for x in ar2:\n            pred_test.append(x)\n    # test_label = test_label.to(DEVICE)\nmultilabel_confusion_matrix(y_test,pred_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Results\nplt.figure(figsize = (8,10))\nplt.plot(list(range(len(output_stats[0]))), output_stats[0], 'g-', label = 'Training loss')\nplt.plot(list(range(len(output_stats[2]))), output_stats[2], 'r-', label = 'Valid loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}