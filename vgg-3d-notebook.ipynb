{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Downloading Libraries","metadata":{}},{"cell_type":"code","source":"!pip install python-gdcm\n!pip install -U pylibjpeg[all]\n!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:34:49.680990Z","iopub.execute_input":"2022-11-25T05:34:49.681705Z","iopub.status.idle":"2022-11-25T05:35:18.508847Z","shell.execute_reply.started":"2022-11-25T05:34:49.681609Z","shell.execute_reply":"2022-11-25T05:35:18.507583Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: python-gdcm in /opt/conda/lib/python3.7/site-packages (3.0.20)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: pylibjpeg[all] in /opt/conda/lib/python3.7/site-packages (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pylibjpeg[all]) (1.21.6)\nRequirement already satisfied: pylibjpeg-rle in /opt/conda/lib/python3.7/site-packages (from pylibjpeg[all]) (1.3.0)\nRequirement already satisfied: pylibjpeg-libjpeg in /opt/conda/lib/python3.7/site-packages (from pylibjpeg[all]) (1.3.2)\nRequirement already satisfied: pylibjpeg-openjpeg in /opt/conda/lib/python3.7/site-packages (from pylibjpeg[all]) (1.2.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: torchsummary in /opt/conda/lib/python3.7/site-packages (1.5.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Importing and Installing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport pathlib\nimport random\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import optim\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport albumentations.pytorch\nfrom tqdm import tqdm\nfrom torchsummary import summary\nfrom sklearn.metrics import multilabel_confusion_matrix\nfrom sklearn.model_selection import StratifiedShuffleSplit\n# Setting Seed for reproducibility\nseed = 42\nrandom.seed(seed)\ntorch.manual_seed(seed)  \ntorch.cuda.manual_seed(seed)  \ntorch.cuda.manual_seed_all(seed)  \ntorch.backends.cudnn.deterministic = True\n\nprint(f\"Torch Version: {torch.__version__}\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:18.512723Z","iopub.execute_input":"2022-11-25T05:35:18.513915Z","iopub.status.idle":"2022-11-25T05:35:20.470237Z","shell.execute_reply.started":"2022-11-25T05:35:18.513870Z","shell.execute_reply":"2022-11-25T05:35:20.469177Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Torch Version: 1.11.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Hyperparamters","metadata":{}},{"cell_type":"code","source":"epochs = 3\nbatch_size = 2\nnumber_of_slices = 160 # Mean of, mean and median of no of slices\ninput_shape = (224, 224)\nresize_size = 256\n# num_of_hidden = 1\n# hidden_dimension = [2048]\noutput_categories = 8\nloss_weights = {'-ve': torch.tensor([1., 1., 1., 1., 1., 1., 1., 7.]),\n                '+ve': torch.tensor([2., 2., 2., 2., 2., 2., 2., 14.])}\ninput_channels = 1\nhidden_cnn_channels = [64, 128, 256, 512, 512]\nblock_cnn_layers = [2,2,3,3,3]\nnum_cnn_block = 5\ncriterion = nn.BCEWithLogitsLoss(reduction = 'none')\ntrain_size = 0.8\nval_size = 0.2\nlr = 0.00005\nsave_path = '/kaggle/working/'","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.471848Z","iopub.execute_input":"2022-11-25T05:35:20.472418Z","iopub.status.idle":"2022-11-25T05:35:20.480360Z","shell.execute_reply.started":"2022-11-25T05:35:20.472379Z","shell.execute_reply":"2022-11-25T05:35:20.479284Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Helper Function","metadata":{}},{"cell_type":"code","source":"def loss_fxn(y_true, y_pred):\n    losses = nn.BCEWithLogitsLoss()\n    loss = losses(y_true,y_pred)\n    weights  = y_true*loss_weights['+ve'].to(device) + (1-y_true)*loss_weights['-ve'].to(device)\n    loss = (loss * weights).sum(axis=1)\n    loss = loss.mean()\n    loss = (loss / weights.sum(axis=1)).sum()\n    return loss\n\ndef metric(y_true, y_pred):\n    losses = nn.BCEWithLogitsLoss()\n    loss = losses(y_true,y_pred)\n    weights  = y_true*loss_weights['+ve'].to(device) + (1-y_true)*loss_weights['-ve'].to(device)\n    loss = (loss * weights).sum(axis=1)\n    loss = loss.mean()\n    loss = (loss / weights.sum(axis=1)).sum()\n    return loss\n\ndef load_model(model, optimizer = None, save_name = 'best_model.pth'):\n    checkpoint = torch.load(save_name)\n    model.load_state_dict(checkpoint['model'])\n    if 'optimizer' in checkpoint and optimizer:\n        optimizer.load_state_dict(checkpoint[optimizer])\n    return model\n\ndef save_model(model, optimizer = None, save_name = 'best_model.pth'):\n    save_dictionary ={}\n    save_dictionary['model'] = model.state_dict()\n    if optimizer:\n        save_dictionary['optimizer'] = optimizer.state_dict()\n    torch.save(save_dictionary, save_name)\n\n\ndef evaluate(model, dataloader):\n    model.eval()\n    with torch.no_grad():\n        metrics = 0.0\n        losses = 0.0\n        for images, labels in tqdm(dataloader):\n            images = images.to(device)\n            labels = labels.to(device)\n            with torch.cuda.amp.autocast():\n                preds = model(images)\n                loss = loss_fxn(preds, labels)\n            met = metric(preds, labels)\n            metrics += met.item()\n            losses += loss.item()\n            \n            gc.collect()\n            torch.cuda.empty_cache()\n        final_loss = losses / len(dataloader)\n        final_metric = metrics / len(dataloader)\n        return final_loss, final_metric","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.484202Z","iopub.execute_input":"2022-11-25T05:35:20.484592Z","iopub.status.idle":"2022-11-25T05:35:20.498972Z","shell.execute_reply.started":"2022-11-25T05:35:20.484555Z","shell.execute_reply":"2022-11-25T05:35:20.497795Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"base_path = pathlib.Path('/kaggle/input/rsna-2022-cervical-spine-fracture-detection')\ndf = pd.read_csv(base_path/'train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.500668Z","iopub.execute_input":"2022-11-25T05:35:20.501041Z","iopub.status.idle":"2022-11-25T05:35:20.529403Z","shell.execute_reply.started":"2022-11-25T05:35:20.501004Z","shell.execute_reply":"2022-11-25T05:35:20.528542Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"            StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7\n0   1.2.826.0.1.3680043.6200                1   1   1   0   0   0   0   0\n1  1.2.826.0.1.3680043.27262                1   0   1   0   0   0   0   0\n2  1.2.826.0.1.3680043.21561                1   0   1   0   0   0   0   0\n3  1.2.826.0.1.3680043.12351                0   0   0   0   0   0   0   0\n4   1.2.826.0.1.3680043.1363                1   0   0   0   0   1   0   0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>patient_overall</th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>C4</th>\n      <th>C5</th>\n      <th>C6</th>\n      <th>C7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.6200</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.27262</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.21561</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.12351</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.1363</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df = df[df['StudyInstanceUID'] != '1.2.826.0.1.3680043.20574'].copy()\nprint(len(df))","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.530812Z","iopub.execute_input":"2022-11-25T05:35:20.531136Z","iopub.status.idle":"2022-11-25T05:35:20.539576Z","shell.execute_reply.started":"2022-11-25T05:35:20.531103Z","shell.execute_reply":"2022-11-25T05:35:20.537142Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"2018\n","output_type":"stream"}]},{"cell_type":"code","source":"df['path'] = list(map(lambda x: base_path/'train_images'/x, df['StudyInstanceUID']))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.541181Z","iopub.execute_input":"2022-11-25T05:35:20.542548Z","iopub.status.idle":"2022-11-25T05:35:20.582322Z","shell.execute_reply.started":"2022-11-25T05:35:20.542508Z","shell.execute_reply":"2022-11-25T05:35:20.581438Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"            StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7  \\\n0   1.2.826.0.1.3680043.6200                1   1   1   0   0   0   0   0   \n1  1.2.826.0.1.3680043.27262                1   0   1   0   0   0   0   0   \n2  1.2.826.0.1.3680043.21561                1   0   1   0   0   0   0   0   \n3  1.2.826.0.1.3680043.12351                0   0   0   0   0   0   0   0   \n4   1.2.826.0.1.3680043.1363                1   0   0   0   0   1   0   0   \n\n                                                path  \n0  /kaggle/input/rsna-2022-cervical-spine-fractur...  \n1  /kaggle/input/rsna-2022-cervical-spine-fractur...  \n2  /kaggle/input/rsna-2022-cervical-spine-fractur...  \n3  /kaggle/input/rsna-2022-cervical-spine-fractur...  \n4  /kaggle/input/rsna-2022-cervical-spine-fractur...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>patient_overall</th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>C4</th>\n      <th>C5</th>\n      <th>C6</th>\n      <th>C7</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.6200</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/rsna-2022-cervical-spine-fractur...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.27262</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/rsna-2022-cervical-spine-fractur...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.21561</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/rsna-2022-cervical-spine-fractur...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.12351</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/rsna-2022-cervical-spine-fractur...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.1363</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/rsna-2022-cervical-spine-fractur...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# min_slices = 1500\n# max_slices = 0\n# mean_slices = 0\n# count = 0\n# slices_val = []\n# for i in df['path']:\n#     no_of_images = len(list(i.glob(\"*\")))\n#     count+=1\n#     slices_val.append(no_of_images)\n#     mean_slices += no_of_images\n#     if min_slices > no_of_images:\n#         min_slices = no_of_images\n#     if no_of_images > max_slices:\n#         max_slices = no_of_images\n# print(\"Minimum Slices in an Image\", min_slices)\n# print(\"Maximum Slices in an Image\", max_slices)\n# print(\"Mean Slices in an Image\", mean_slices / count)\n# slices_val.sort()\n# print(\"Median Slices in an Image\", slices_val[(count + 1)//2])","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.583630Z","iopub.execute_input":"2022-11-25T05:35:20.584090Z","iopub.status.idle":"2022-11-25T05:35:20.589646Z","shell.execute_reply.started":"2022-11-25T05:35:20.584051Z","shell.execute_reply":"2022-11-25T05:35:20.588686Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"strat = StratifiedShuffleSplit(n_splits=2, test_size = val_size/(train_size + val_size), \n                                random_state=seed)\nfor (train_idx, valid_idx) in strat.split(df.index, df['C3']):\n    valid_data = df.iloc[valid_idx]\n    train_data = df.iloc[train_idx]","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.591272Z","iopub.execute_input":"2022-11-25T05:35:20.591999Z","iopub.status.idle":"2022-11-25T05:35:20.602326Z","shell.execute_reply.started":"2022-11-25T05:35:20.591963Z","shell.execute_reply":"2022-11-25T05:35:20.601381Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"len(train_data), len(valid_data)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.604001Z","iopub.execute_input":"2022-11-25T05:35:20.604391Z","iopub.status.idle":"2022-11-25T05:35:20.613380Z","shell.execute_reply.started":"2022-11-25T05:35:20.604357Z","shell.execute_reply":"2022-11-25T05:35:20.612147Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(1614, 404)"},"metadata":{}}]},{"cell_type":"code","source":"train_data['patient_overall'].value_counts(normalize = True)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.615807Z","iopub.execute_input":"2022-11-25T05:35:20.616562Z","iopub.status.idle":"2022-11-25T05:35:20.626297Z","shell.execute_reply.started":"2022-11-25T05:35:20.616524Z","shell.execute_reply":"2022-11-25T05:35:20.625130Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0    0.521066\n1    0.478934\nName: patient_overall, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"valid_data['patient_overall'].value_counts(normalize = True)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.627992Z","iopub.execute_input":"2022-11-25T05:35:20.628423Z","iopub.status.idle":"2022-11-25T05:35:20.638288Z","shell.execute_reply.started":"2022-11-25T05:35:20.628387Z","shell.execute_reply":"2022-11-25T05:35:20.636983Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0    0.537129\n1    0.462871\nName: patient_overall, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"class CervicalDataset(Dataset):\n    def __init__(self, df, no_of_slice, prob = 0.5, test = False, tta = False):\n        '''\n        '''\n        self.df = df\n        self.test = test\n        self.tta = tta\n        self.prob = prob\n        self.no_of_slice = no_of_slice\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        data = self.df['path'].iloc[idx]\n        images_path = list(data.glob(\"*\"))\n        no_of_images = len(images_path)\n        transforms_list = [A.LongestMaxSize(max_size=resize_size, interpolation=1),\n                           A.PadIfNeeded(min_height=input_shape[0], min_width=input_shape[1],\n                                         border_mode=0, value=(0,0,0))]\n        if not(self.test):\n            value = np.random.uniform()\n            if value >= self.prob: \n                transforms_list.append(A.HorizontalFlip(always_apply = True))\n                \n            value = np.random.uniform()\n            if value >= self.prob:\n                transforms_list.append(A.ShiftScaleRotate(shift_limit = 0.15,\n                                                          border_mode = 2, always_apply = True))\n            \n            # Other Transformation to add: Random Brightness, Contrast, Clahe, Scale Intensity \n                \n            transforms_list.append(A.CenterCrop(height = input_shape[0], width = input_shape[1]))\n            transforms = A.Compose(transforms_list)\n        elif self.test and self.tta:\n            pass\n        if self.test:\n            transforms_list.append(A.CenterCrop(height = input_shape[0], width = input_shape[1]))\n            transforms = A.Compose(transforms_list)\n            \n        imgs = []\n        for i in range(1, len(images_path)+1):\n            path = images_path[0].parent/f\"{i}.dcm\"\n            \n            try:\n                data = pydicom.dcmread(path)\n#                 data.PhotometricInterpretation = 'YBR_FULL'\n                img_data = apply_voi_lut(data.pixel_array, data)\n                img_data = img_data - np.min(img_data)\n                if np.max(img_data) != 0:\n                    img_data = img_data / np.max(img_data)\n    #             data = (data * 255).astype(np.uint8)\n                album = transforms(image = img_data)\n                img_data = album['image']\n            except FileNotFoundError:\n                continue\n            imgs.append(img_data)\n            if len(imgs) > self.no_of_slice:\n                break\n            \n        if len(imgs) > self.no_of_slice:\n            imgs = imgs[:self.no_of_slice]\n        \n        if len(imgs) < self.no_of_slice:\n            imgs.extend([np.zeros((input_shape[0], input_shape[1]))\n                         for i in range(self.no_of_slice - len(imgs))])\n\n        imgs = np.array(imgs)\n        imgs = torch.from_numpy(imgs).float()\n        labels = torch.as_tensor(self.df.iloc[idx].iloc[1:-1]).float()\n        imgs = torch.unsqueeze(imgs, dim = 0)\n        return imgs, labels","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.642567Z","iopub.execute_input":"2022-11-25T05:35:20.642828Z","iopub.status.idle":"2022-11-25T05:35:20.659498Z","shell.execute_reply.started":"2022-11-25T05:35:20.642804Z","shell.execute_reply":"2022-11-25T05:35:20.658640Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_dataset = CervicalDataset(train_data, no_of_slice = number_of_slices)\nvalid_dataset = CervicalDataset(valid_data, no_of_slice = number_of_slices, test = True)\ntrainloader = DataLoader(train_dataset, batch_size = batch_size)\nvalidloader = DataLoader(valid_dataset, batch_size = batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.662090Z","iopub.execute_input":"2022-11-25T05:35:20.662685Z","iopub.status.idle":"2022-11-25T05:35:20.670960Z","shell.execute_reply.started":"2022-11-25T05:35:20.662649Z","shell.execute_reply":"2022-11-25T05:35:20.669984Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# # Visualize\n# image = imgs[3]\n# print(image.shape) # No_of_slice x height x width\n# fig, axes = plt.subplots(nrows = 1, ncols = 4, figsize = (15,10))\n# axes[0].imshow(image.numpy().mean(axis = 0), cmap = 'gray')\n# axes[0].axis('off')\n# axes[1].imshow(image.numpy()[:, :, image.shape[2]//2], cmap = 'gray')\n# axes[1].axis('off')\n# axes[2].imshow(image.numpy()[:, 128, :], cmap = 'gray')\n# axes[2].axis('off')\n# axes[3].imshow(image.numpy()[6, :, :], cmap = 'gray')\n# axes[3].axis('off');","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.672315Z","iopub.execute_input":"2022-11-25T05:35:20.672749Z","iopub.status.idle":"2022-11-25T05:35:20.681827Z","shell.execute_reply.started":"2022-11-25T05:35:20.672711Z","shell.execute_reply":"2022-11-25T05:35:20.680812Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"class VGGBlock(nn.Module):\n    def __init__(self, input_channels, output_channels, num_cnn = 2):\n        super(VGGBlock, self).__init__()\n        layers = []\n        for i in range(num_cnn):\n            layers.append(nn.Conv3d(in_channels = input_channels, out_channels = output_channels, \n                                   kernel_size = 3, padding = 1))\n            layers.append(nn.ReLU())\n            # Change after 1st layer\n            input_channels = output_channels\n            \n        layers.append(nn.MaxPool3d(kernel_size = 2))\n        self.base_module = nn.Sequential(*layers)\n        self.__initialise_weights()\n        \n    def __initialise_weights(self):\n        for m in self.base_module:\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight)\n              \n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n                    \n    def forward(self, x):\n        out = self.base_module(x)\n        return out\n    \nclass VGGNet(nn.Module):\n    def __init__(self, input_channels, output_categories, hidden_channels, \n                 block_cnn_layers , num_cnn_block = 5, learning_rate = lr):\n        super(VGGNet, self).__init__()\n        \n        hidden_cnn_channels = hidden_channels[:]\n            \n        if (num_cnn_block != len(hidden_cnn_channels)):\n            raise \"Number of Hidden layer and length of hidden dim must be same\"\n\n        self.learning_rate = learning_rate\n        \n        \n        # Dropout probability\n        self.p = 0.5\n        hidden_cnn_channels.insert(0, input_channels)\n        vgg_layers = []\n        for i in range(1,len(hidden_cnn_channels)):\n            vgg_layers.append(VGGBlock(hidden_cnn_channels[i-1], \n                                       hidden_cnn_channels[i], \n                                       block_cnn_layers[i-1]))\n            \n        vgg_layers.append(nn.AdaptiveAvgPool3d((7,1,1)))\n        self.network = nn.Sequential(*vgg_layers)\n\n        self.classification = nn.Sequential(nn.Linear(3584, output_categories))\n        self.__initialise_weights()\n\n    def __initialise_weights(self):\n        for m in self.classification:\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight)\n              \n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n        \n    def forward(self,x):\n        out = self.network(x)\n        out = out.view(batch_size, -1)\n        out = self.classification(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.683305Z","iopub.execute_input":"2022-11-25T05:35:20.683837Z","iopub.status.idle":"2022-11-25T05:35:20.699648Z","shell.execute_reply.started":"2022-11-25T05:35:20.683803Z","shell.execute_reply":"2022-11-25T05:35:20.698864Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def fit(model, n_epochs, trainloader, validloader, \n        optimizer, scheduler = None, early_stopping = False, \n        early_stopping_threshold = 5, min_metric_score = 100,\n        save_path = 'best_model.pth'):\n    model = model.to(device)\n    train_losses = []\n    train_metric = []\n    valid_losses = []\n    valid_metric = []\n    scaler = torch.cuda.amp.GradScaler()\n    no_change = 0\n\n    for epoch in range(n_epochs):\n        loop = tqdm(enumerate(trainloader), total = len(trainloader), leave = False)\n        metric_scores = 0.0\n        losses = 0.0\n        model.train()\n        for idx, (images, labels) in loop:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # For FP16 (Ref: https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/)\n            with torch.cuda.amp.autocast():\n                preds = model(images)\n                loss = loss_fxn(preds, labels)\n\n            optimizer.zero_grad()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            gc.collect()\n\n            metric_score = metric(preds, labels)\n            metric_scores += metric_score.item()\n            losses += loss.item()\n            loop.set_description(f\"[{epoch}/{n_epochs}]\")\n            loop.set_postfix(train_loss = loss.item(), train_metric = metric_score.item())\n            \n            torch.cuda.empty_cache()\n            \n        \n        batch_loss_train = losses/len(trainloader)\n        batch_metric_train = metric_scores/len(trainloader)\n        train_losses.append(batch_loss_train)\n        train_metric.append(batch_metric_train)\n\n        valid_loss, valid_score = evaluate(model, validloader)\n        \n        valid_losses.append(valid_loss)\n        valid_metric.append(valid_score)\n        print(f\"[{epoch}/{n_epochs}]\")\n        print(f\"train_loss = {batch_loss_train}, train_metric = {batch_metric_train}, valid_loss = {valid_loss}, valid_metric = {valid_score}\")\n        if scheduler:\n            scheduler.step()\n        if valid_score < min_metric_score:\n            print(\"<<<< Saving >>>>\")\n            no_change = 0\n            min_dice_score = valid_score\n            save_model(model, optimizer, save_path)\n\n        else:\n            no_change+=1\n        \n        if no_change > early_stopping_threshold and early_stopping:\n            print('###### Early Stopping ######')\n            break\n    return train_losses, train_metric, valid_losses, valid_metric","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.701069Z","iopub.execute_input":"2022-11-25T05:35:20.701418Z","iopub.status.idle":"2022-11-25T05:35:20.716338Z","shell.execute_reply.started":"2022-11-25T05:35:20.701384Z","shell.execute_reply":"2022-11-25T05:35:20.715399Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model = VGGNet(input_channels, output_categories, hidden_cnn_channels, block_cnn_layers, num_cnn_block).to(device)\n\n# print(\"Model's state_dict:\")\n# for param_tensor in model.state_dict():\n#     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n    \n# def count_parameters(model):\n#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nsummary(model, (1, 160, 224, 224))\n\n\n# print(f'The model has {count_parameters(model):,} trainable parameters')","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:35:20.717813Z","iopub.execute_input":"2022-11-25T05:35:20.718229Z","iopub.status.idle":"2022-11-25T05:35:27.326934Z","shell.execute_reply.started":"2022-11-25T05:35:20.718182Z","shell.execute_reply":"2022-11-25T05:35:27.325718Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv3d-1    [-1, 64, 160, 224, 224]           1,792\n              ReLU-2    [-1, 64, 160, 224, 224]               0\n            Conv3d-3    [-1, 64, 160, 224, 224]         110,656\n              ReLU-4    [-1, 64, 160, 224, 224]               0\n         MaxPool3d-5     [-1, 64, 80, 112, 112]               0\n          VGGBlock-6     [-1, 64, 80, 112, 112]               0\n            Conv3d-7    [-1, 128, 80, 112, 112]         221,312\n              ReLU-8    [-1, 128, 80, 112, 112]               0\n            Conv3d-9    [-1, 128, 80, 112, 112]         442,496\n             ReLU-10    [-1, 128, 80, 112, 112]               0\n        MaxPool3d-11      [-1, 128, 40, 56, 56]               0\n         VGGBlock-12      [-1, 128, 40, 56, 56]               0\n           Conv3d-13      [-1, 256, 40, 56, 56]         884,992\n             ReLU-14      [-1, 256, 40, 56, 56]               0\n           Conv3d-15      [-1, 256, 40, 56, 56]       1,769,728\n             ReLU-16      [-1, 256, 40, 56, 56]               0\n           Conv3d-17      [-1, 256, 40, 56, 56]       1,769,728\n             ReLU-18      [-1, 256, 40, 56, 56]               0\n        MaxPool3d-19      [-1, 256, 20, 28, 28]               0\n         VGGBlock-20      [-1, 256, 20, 28, 28]               0\n           Conv3d-21      [-1, 512, 20, 28, 28]       3,539,456\n             ReLU-22      [-1, 512, 20, 28, 28]               0\n           Conv3d-23      [-1, 512, 20, 28, 28]       7,078,400\n             ReLU-24      [-1, 512, 20, 28, 28]               0\n           Conv3d-25      [-1, 512, 20, 28, 28]       7,078,400\n             ReLU-26      [-1, 512, 20, 28, 28]               0\n        MaxPool3d-27      [-1, 512, 10, 14, 14]               0\n         VGGBlock-28      [-1, 512, 10, 14, 14]               0\n           Conv3d-29      [-1, 512, 10, 14, 14]       7,078,400\n             ReLU-30      [-1, 512, 10, 14, 14]               0\n           Conv3d-31      [-1, 512, 10, 14, 14]       7,078,400\n             ReLU-32      [-1, 512, 10, 14, 14]               0\n           Conv3d-33      [-1, 512, 10, 14, 14]       7,078,400\n             ReLU-34      [-1, 512, 10, 14, 14]               0\n        MaxPool3d-35         [-1, 512, 5, 7, 7]               0\n         VGGBlock-36         [-1, 512, 5, 7, 7]               0\nAdaptiveAvgPool3d-37         [-1, 512, 7, 1, 1]               0\n           Linear-38                    [-1, 8]          28,680\n================================================================\nTotal params: 44,160,840\nTrainable params: 44,160,840\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 30.62\nForward/backward pass size (MB): 22786.94\nParams size (MB): 168.46\nEstimated Total Size (MB): 22986.03\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.AdamW(model.parameters(), lr = lr) \n# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=lr, max_lr=max_lr,\n#                                         step_size_up=10,mode=\"exp_range\",gamma=0.9, cycle_momentum = False)\n\noutput_stats = fit(model, epochs, trainloader, validloader, optimizer, early_stopping = False, \n                   early_stopping_threshold = 3, save_path = f'/kaggle/working/vgg3d.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model(model, save_name = '/kaggle/working/vgg3d.pt')\ny_test=[]\npred_test=[]\nsig = nn.Sigmoid()\nmodel.eval()\nwith torch.no_grad():\n    for data in tqdm(validloader):\n        test_image, test_label=data\n        test_image = test_image.to(device)\n        with torch.cuda.amp.autocast():\n            y_pred = model(test_image)\n            y_pred = sig(y_pred)\n#         print(y_pred)\n        y_pred[y_pred>=0.50]=1\n        y_pred[y_pred<0.50]=0\n        ar1=test_label.cpu().data.numpy()\n        ar2=y_pred.cpu().data.numpy()\n        for x in ar1:\n            y_test.append(x)\n        for x in ar2:\n            pred_test.append(x)\n    # test_label = test_label.to(DEVICE)\nmultilabel_confusion_matrix(y_test,pred_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Results\nplt.figure(figsize = (8,10))\nplt.plot(list(range(len(output_stats[0]))), output_stats[0], 'g-', label = 'Training loss')\nplt.plot(list(range(len(output_stats[2]))), output_stats[2], 'r-', label = 'Valid loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend();","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}